{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "A US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n",
    "\n",
    "They have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n",
    "\n",
    "- Which variables are significant in predicting the demand for shared bikes.\n",
    "- How well those variables describe the bike demands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import pandas as pd,numpy as np\n",
    "import matplotlib.pyplot as plt,seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reading & Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "df=pd.read_csv('day.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>01-01-2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14.110847</td>\n",
       "      <td>18.18125</td>\n",
       "      <td>80.5833</td>\n",
       "      <td>10.749882</td>\n",
       "      <td>331</td>\n",
       "      <td>654</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>02-01-2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14.902598</td>\n",
       "      <td>17.68695</td>\n",
       "      <td>69.6087</td>\n",
       "      <td>16.652113</td>\n",
       "      <td>131</td>\n",
       "      <td>670</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>03-01-2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.050924</td>\n",
       "      <td>9.47025</td>\n",
       "      <td>43.7273</td>\n",
       "      <td>16.636703</td>\n",
       "      <td>120</td>\n",
       "      <td>1229</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>04-01-2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>10.60610</td>\n",
       "      <td>59.0435</td>\n",
       "      <td>10.739832</td>\n",
       "      <td>108</td>\n",
       "      <td>1454</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>05-01-2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.305237</td>\n",
       "      <td>11.46350</td>\n",
       "      <td>43.6957</td>\n",
       "      <td>12.522300</td>\n",
       "      <td>82</td>\n",
       "      <td>1518</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  holiday  weekday  workingday  \\\n",
       "0        1  01-01-2018       1   0     1        0        6           0   \n",
       "1        2  02-01-2018       1   0     1        0        0           0   \n",
       "2        3  03-01-2018       1   0     1        0        1           1   \n",
       "3        4  04-01-2018       1   0     1        0        2           1   \n",
       "4        5  05-01-2018       1   0     1        0        3           1   \n",
       "\n",
       "   weathersit       temp     atemp      hum  windspeed  casual  registered  \\\n",
       "0           2  14.110847  18.18125  80.5833  10.749882     331         654   \n",
       "1           2  14.902598  17.68695  69.6087  16.652113     131         670   \n",
       "2           1   8.050924   9.47025  43.7273  16.636703     120        1229   \n",
       "3           1   8.200000  10.60610  59.0435  10.739832     108        1454   \n",
       "4           1   9.305237  11.46350  43.6957  12.522300      82        1518   \n",
       "\n",
       "    cnt  \n",
       "0   985  \n",
       "1   801  \n",
       "2  1349  \n",
       "3  1562  \n",
       "4  1600  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(730, 16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view initial shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 730 entries, 0 to 729\n",
      "Data columns (total 16 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   instant     730 non-null    int64  \n",
      " 1   dteday      730 non-null    object \n",
      " 2   season      730 non-null    int64  \n",
      " 3   yr          730 non-null    int64  \n",
      " 4   mnth        730 non-null    int64  \n",
      " 5   holiday     730 non-null    int64  \n",
      " 6   weekday     730 non-null    int64  \n",
      " 7   workingday  730 non-null    int64  \n",
      " 8   weathersit  730 non-null    int64  \n",
      " 9   temp        730 non-null    float64\n",
      " 10  atemp       730 non-null    float64\n",
      " 11  hum         730 non-null    float64\n",
      " 12  windspeed   730 non-null    float64\n",
      " 13  casual      730 non-null    int64  \n",
      " 14  registered  730 non-null    int64  \n",
      " 15  cnt         730 non-null    int64  \n",
      "dtypes: float64(4), int64(11), object(1)\n",
      "memory usage: 91.4+ KB\n"
     ]
    }
   ],
   "source": [
    "#check info about dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical description\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping irrelevant features\n",
    " - instant is index - we can drop it\n",
    " - dteday is conveying the same meaning as yr & mnth\n",
    " - casual & registered can be removed as it totals to the target value 'cnt' & possibly be a reason of data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['instant','dteday','casual','registered'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "#### Mapping data according to dictionary\n",
    "this step is done prior to visualization so that it can have meaningful item names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.season = df.season.map({1: 'Spring',2:'Summer',3:'Fall',4:'Winter'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.yr = df.yr.map({0: '2018',1:'2019'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "df.mnth = df.mnth.apply(lambda x:calendar.month_name[x])\n",
    "df.mnth = df.mnth.apply(lambda x:x[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.weekday = df.weekday.map({0:\"Sun\",1:\"Mon\",2:\"Tue\",3:\"Wed\",4:\"Thr\",5:\"Fri\",6:\"Sat\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.weathersit = df.weathersit.map({1:'Clear_FewClouds_PartlyCloudy',\n",
    "                                   2:'Mist_CloudyMist_BrokenClouds_FewClouds', \n",
    "                                   3:'LightSnow_LightRain_Thunderstorm_ScatteredClouds',\n",
    "                                   4:'HeavyRain_IcePallets_Thunderstorm_Mist_Snow_Fog'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we categorize features into categoricals & continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_f=['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday',\n",
    "       'weathersit']\n",
    "cont_f=['temp', 'atemp', 'hum', 'windspeed','cnt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in list(enumerate(cont_f)):\n",
    "    plt.subplot(3,2, i[0]+1)\n",
    "    sns.boxplot(df[i[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences\n",
    "- There are some outliers in the hum & windspeed but it does not seem to effect much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(2,2,1)\n",
    "sns.barplot(x=df.season,y=df.cnt,hue=df.yr,data=df)\n",
    "plt.subplot(2,2,2)\n",
    "sns.barplot(x=df.mnth,y=df.cnt,hue=df.yr,data=df)\n",
    "plt.subplot(2,2,4)\n",
    "sns.barplot(y=df.weathersit,x=df.cnt,hue=df.yr,data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences\n",
    "- The overall trend increases in 2019\n",
    "- Summer,winter & fall sees good rentals\n",
    "- the trend increases from march then dips in december\n",
    "- rentals is low for LightSnow_LightRain_Thunderstorm_ScatteredClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(df.corr(), annot = True, cmap=\"Greens\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferences\n",
    "- The temp & atemp is highly correlated but we will not drop them now, we will let the model decide which is more important\n",
    "- hum, holiday & windspeed is negatively correlated with cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Variables\n",
    "\n",
    "Assumption 1 - Linear relationship between X & Y\n",
    "\n",
    "The relationship is not exactly linear but temp & atemp does show some linear relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df[cont_f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,15))\n",
    "for i in list(enumerate(cat_f)):\n",
    "    plt.subplot(4,2, i[0]+1)\n",
    "    sns.boxplot(x=df[i[1]],y=df.cnt,data=df)\n",
    "    plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences\n",
    "- The number of rentals increased in 2019\n",
    "- The rentals increase from summer & decreases towards winter but is still higher than Spring\n",
    "- The rentals increases from Aprill but dips in July & is most in september\n",
    "- There is a dip during holidays & non workingday\n",
    "- There is no rentals during Heavy Rain or Ice pallets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Contd.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Variables\n",
    "Creating dummy variables for categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy=df[['season','mnth','weathersit','weekday','yr']]\n",
    "dummy=pd.get_dummies(dummy,drop_first=True)\n",
    "\n",
    "df=pd.concat([df,dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['season','mnth','weathersit','weekday','yr'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#present shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derived Column\n",
    "Created one derived feature 'Windchill Factor' but removed it since it was left out in RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "Splitting the dataset into train & test in 70:30 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "df_train,df_test=train_test_split(df,train_size=0.7,test_size=0.3,random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "Scaling the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[cont_f]=scaler.fit_transform(df_train[cont_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the min value is 0 & max is 1 after scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing data into dependent & independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=df_train.pop('cnt')\n",
    "x_train=df_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modelling and Evaluaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will be building the model with all the given features in the dataset & remove them 1 by 1 using a combo of RFE & VIF/P-Val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression Model\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "#dropping down features count to 15 from 30 using RFE\n",
    "rfe = RFE(lr, 15)           \n",
    "rfe = rfe.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking features selected by RFE\n",
    "rfe_cols=x_train.columns[rfe.support_]\n",
    "rfe_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now using these columns we will proceed further & manually remove element as per statistical decisions following the statsmodel approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train[rfe_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for model creation\n",
    "def model(x_train,y_train):\n",
    "    x_train=sm.add_constant(x_train)\n",
    "    lr=sm.OLS(y_train,x_train).fit()\n",
    "    return lr,x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIF function\n",
    "def VIF(x_train):\n",
    "    X = x_train\n",
    "    features = X.columns\n",
    "    Vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    vif=pd.DataFrame({'Features':features,'VIF':Vif})\n",
    "    vif['VIF'] = round(vif['VIF'], 2)\n",
    "    vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "    return vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding constant & fitting model\n",
    "lr,x_train=model(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIF\n",
    "VIF(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing holiday since its P value is beyond 5%  standing at 37.3% so it is clearly insignificant & VIF is inf which means it is perfectly collinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.drop(['holiday'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebuilding without holiday\n",
    "lr2,x_train=model(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the P values the selected features are all significant so we will check for multicollinearity using VIF(<5 - Good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIF\n",
    "VIF(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see many features having VIF>5 even though the P val made it significant. We will remove the 'workingday' & check again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.drop(['workingday'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebuilding without workingday\n",
    "lr3,x_train=model(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIF\n",
    "VIF(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we saw weekday_sat having a reducing VIF but the significance no longer holds true as P val is 0.218>0.05 so we remove it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.drop(['weekday_Sat'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebuilding without weekday_Sat)\n",
    "lr4,x_train=model(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIF\n",
    "VIF(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "season_Spring is having significance with p val 0.006 but has VIF>5 so we will drop it & rebuild model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5\n",
    "Final Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.drop(['season_Spring'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebuilding without weekday_Sat\n",
    "lr5,x_train=model(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIF\n",
    "VIF(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see the P val is below 5% for all the features & also there is no issue of multicollinearity present so we can finalize this model & proceed with prediction.<br>The Adj R2 obtained on training data is 0.836 we need to compare it with test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Analysis on Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be checking the error terms if they are normally distributed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train=lr5.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution plot of error terms\n",
    "plt.suptitle(\"Residual Analysis on train data- Error Terms\",fontsize=15)\n",
    "sns.distplot((y_train-y_pred_train))\n",
    "plt.xlabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption 2 - The error terms are normally distributed\n",
    "<br>so we can proceed with it to predict on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homoscedasticity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution of error terms to check constant variance\n",
    "plt.suptitle(\"Homscedasticity Check\",fontsize=15)\n",
    "a=sns.scatterplot(y_pred_train,(y_train-y_pred_train))\n",
    "a=sns.lineplot([0,1],[0,0],color='red')\n",
    "plt.xlabel(\"y_pred\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption 3- Error terms have constant variance\n",
    "\n",
    "there is no visible pattern so it can be confirmed it contains homscedascity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "Scaling Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[cont_f]=scaler.transform(df_test[cont_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate out target feature\n",
    "y_test=df_test.pop('cnt')\n",
    "x_test=df_test\n",
    "N=len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the same features in test data as on training data\n",
    "x_train=x_train.drop(['const'],axis=1)\n",
    "x_test=x_test[x_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding constant term\n",
    "x_test=sm.add_constant(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on test data\n",
    "y_pred=lr5.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R2 score on test data\n",
    "r2_test=round(r2_score(y_test,y_pred),4)\n",
    "print('The test data r2 score is : ', r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adj r2 score for test data\n",
    "#N= len(N)          # sample size\n",
    "p =len(x_train.columns)     # Number of independent variable\n",
    "r2_test_adj = round((1-((1-r2_test)*(N-1)/(N-p-1))),4)\n",
    "print('Adj. R-Squared for Test dataset: ', r2_test_adj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can explain 79.30% variance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual vs Predicted\n",
    "fig=plt.figure()\n",
    "ax1=fig.add_subplot(111)\n",
    "plt.suptitle('Actual vs Predicted',fontsize=15)\n",
    "#ax1.scatter(y_test,c='b',label='y_test')\n",
    "#ax1.scatter(y_pred,c='r',label='y_pred')\n",
    "sns.regplot(y_test,y_pred)\n",
    "plt.xlabel(\"y_test\",fontsize=15)\n",
    "plt.ylabel(\"y_pred\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual & predicted gets overlapped which shows the model can predict the change in actual data in a good way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe of features & coefficients\n",
    "coef=pd.DataFrame(lr5.params)\n",
    "coef.insert(0,'Features',coef.index)\n",
    "coef.rename(columns={0:'Coefficient'},inplace=True)\n",
    "coef.reset_index(drop=True,inplace=True)\n",
    "coef.sort_values(by='Coefficient',ascending=False,inplace=True)\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The top indicators of explaning the demand are \n",
    "- temp\n",
    "- weathersit_LightSnow_LightRain_Thunderstorm_ScatteredClouds\n",
    "- yr_2019\n",
    "\n",
    "#### Apart from those some other indicators are\n",
    "- hum\n",
    "- windspeed\n",
    "- season (mainly in summer & winter)\n",
    "- month(sep)\n",
    "\n",
    "BoomBikes can take above indicators into consideration & offer business benifits to attract more customers & gain profit\n",
    "\n",
    "The Positive coefficients depicts the increase in count for those parameters & Negative coefficient signifies decrease in count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Equation of the best fit line obtained is\n",
    "\n",
    "cnt = 0.598625 * temp + 0.228436 * yr_2019 + 0.135878 * season_Winter + 0.091479 * mnth_Sep + 0.082251 * season_Summer + (-0.041966) * weekday_Sun  + (-0.043909) * mnth_Jul + (-0.052904) * weathersit_Mist_CloudyMist_BrokenClouds_FewClouds + (-0.174126) * hum + (-0.189487) * windspeed + (-0.235432) * weathersit_LightSnow_LightRain_Thunderstorm_ScatteredClouds + 0.223111\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences\n",
    "\n",
    "- Temperature is a deciding factor with high coefficient\n",
    "- Rentals are high during summer & winter\n",
    "- Month of september sees a rise in rentals while dips in July\n",
    "- Humidity & windspeed acts as a hindrance to rentals\n",
    "- Weather situation for mist,snow,clouds & rain also acts negatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
